{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a376596-5e48-4277-be70-a615d66cde50",
   "metadata": {},
   "source": [
    "## About this Part\n\nCongratulations! You've reached the final Part of this Sprint.\nThis Part serves as an integrative experience, allowing you to apply the knowledge and skills acquired in this and previous Sprints.\n\nAs the culmination of this Sprint, you'll set up a data modeling pipeline for Formula 1 data.\nSprint projects often necessitate using skills, tools, and techniques not explicitly covered during the Sprint.\nThis is intentional, as true expertise stems from the ability to recognize the skills needed to solve a given problem and to acquire these skills as necessary.\n\nRemember, perfection isn't expected at this stage.\nYou will continuously hone your skills and have ample opportunities to apply them in future projects.\nFor now, focus on leveraging what you've learned and giving it your best effort!\n\n*Note:* [advice on building your portfolio](https://turingcollege.atlassian.net/wiki/spaces/DLG/pages/1002307695/Portfolio+Items)\n\n## Context\n\nThe small boutique consulting firm specializing in data science and engineering that you founded with your friends had a really tough time finding clients in the FinTech sector.\nIt turns out that everybody has their proprietary creditworthiness models, and no one wants to share the data.\nGiven the recent interest and hype around Formula 1, you've decited to think outside of the box and pivoted to a more exotic type of business - specifically Formula 1 teams.\nYou decided to make a **demo application** of a data warehouse, showcasing your data modeling and data engineering skills.\nYou decided to use [this Kaggle repository](https://www.kaggle.com/datasets/rohanrao/formula-1-world-championship-1950-2020/data) as your source data, to ingest it into your data warehouse using traditional techniques and ensure it adheres to the best practices of dimensional data modeling, and finally, using dbt, to transform it from raw to intermediate & intermediate to data marts which could be later used by analysts and decision makers in F1 teams.\n\nNote: install dbt [using Docker](https://docs.getdbt.com/docs/core/docker-install). You are free to choose the database technology yourself.\n\n## Objectives for this Part\n\n- Practice using dimensional data modeling.\n- Practice data denormalization.\n- Practice using dbt.\n- Practice using Docker.\n\n## Requirements\n\n- Your solution should encompass the functionality outlined in the Context section.\n- Provide suggestions on how your analysis can be improved.\n\n## Evaluation Criteria\n\n- Adherence to the requirements. How well did you meet the requirements?\n- Code quality. Was your code well-structured? Did you use the appropriate levels of abstraction? Did you remove commented-out and unused code?\n- System design. Did your solution use suitable technologies, tools, software architecture, and algorithms?\n- Presentation quality. How comprehensive is your presentation, and how well are you able to explain your solution to the target audience?\n- Conceptual understanding. How well do you know the concepts covered in this and previous Sprints?\n\n## Project Review\n\nDuring your project review, you should present it as if talking to a potential client for whom you are showcasing your solution.  \nYou can assume that they will have some software and data engineering skills - they will understand technical jargon, but are not expected to notice and question things that could have been done better or ask about the choices you've made.\nThey are very familiar with the business domain and are looking for consultants that could help them with architecting and engineering.\n\nDuring the presentation, you might be asked questions to test your understanding of the covered topics, such as:\n\n- What is the primary objective of dimensional data modeling?\n- What is a star schema?\n- What is a snowflake schema?\n- How does the snowflake schema differ from a star schema? In what circumstances is it appropriate to use each of them?\n- In a star schema, how are dimension tables connected to the fact table?\n- What is a slowly changing dimension in dimensional data modeling?\n- What is the purpose of the bridge table in dimensional data modeling?\n- What is the process of denormalization?\n\nIMPORTANT: during the project review, you might also be asked to solve an exercise using Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2be2b6-1b40-48b1-a201-0206c926f0a3",
   "metadata": {},
   "source": [
    "## General Project Review Guidelines\n\nFor an in-depth explanation about how project reviews work at Turing College, please read [this doc](https://turingcollege.atlassian.net/wiki/spaces/DLG/pages/537395951/Peer+expert+reviews+corrections).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
